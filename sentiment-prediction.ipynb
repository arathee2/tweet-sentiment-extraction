{"cells":[{"metadata":{"_uuid":"0ccebf86-5fc5-414a-b431-77a3659658de","_cell_guid":"26ad68c6-dd44-427f-b7c7-d7bc677c7764","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import regularizers\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Input\nfrom keras.layers import TimeDistributed\nfrom keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN, Dropout\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport urllib.request\nimport requests\nimport collections\nfrom nltk import FreqDist\nfrom collections import Counter\nimport seaborn as sns\nimport requests\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_colwidth', 100)\n%config InlineBackend.figure_format='retina'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\nCLEAN TWEETS\n'''\n\ntrain_dir = '../input/tweet-sentiment-extraction/train.csv'\ntweets_train = pd.read_csv(train_dir, encoding='utf_8')\n\ntest_dir = '../input/tweet-sentiment-extraction/test.csv'\ntweets_test = pd.read_csv(test_dir, encoding='utf_8')\n\nprint(tweets_train.shape)\ntweets_train[['text', 'sentiment']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_word_length(text):\n    words = text.split(' ')\n    word_lengths = [len(word) for word in words]\n    sns.boxplot(word_lengths)\n    pass\n\nplt.figure(figsize=(12, 5))\nplot_word_length(document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean text\ndef clean_tweet(tweet):\n    \n    # make str\n    tweet = str(tweet)\n    \n    # lower case\n    tweet = tweet.lower()\n    \n    # remove numbers\n    #tweet = re.sub(r\"\\d+\", \" \", tweet)\n    \n    # remove urls\n    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n    tweet = re.sub(url_pattern, ' ', tweet)\n    \n    # remove html\n    html = r'<.*?>'\n    tweet = re.sub(html, ' ', tweet)\n    \n    # remove puncuations and numbers\n    tweet = re.sub(r\"\"\"[^\\s\\w\\d'\"]+\"\"\", ' ', tweet)\n    \n    # expand contractions\n    tweet = expand_contractions(tweet)\n    \n    # remove extra whitespaces\n    tweet = re.sub('\\s+', ' ', tweet)\n    tweet = tweet.strip()\n    \n    # tokenize\n    words = word_tokenize(tweet)\n    \n    # remove whitespaces\n    words = [word.strip() for word in words]\n    \n    # remove words shorter than n characters in length\n    n = 1\n    words = [word for word in words if len(word) > n]\n    \n    # lemmatize\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    \n    # join words\n    tweet = ' '.join(words)\n    \n    return tweet\n\ndef expand_contractions(text):\n    contractions = { \n    \"ain't\": \"am not\",\n    \"aint\": \"am not\",\n    \"aren't\": \"are not\",\n    \"arent\": \"are not\",\n    \"can't\": \"cannot\",\n        \"cant\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldnt\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"didnt\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"doesnt\": \"does not\",\n    \"don't\": \"do not\",\n    \"dont\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadnt\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"hasnt\": \"has not\",\n    \"haven't\": \"have not\",\n    \"havent\": \"have not\",\n    \"he'd\": \"he would\",\n    \"hed\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"Id\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"Im\": \"I am\",\n    \"I've\": \"I have\",\n    \"Ive\": \"I have\",\n    \"isn't\": \"is not\",\n    \"isnt\": \"is not\",\n    \"it'd\": \"it would\",\n    \"itd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"lets\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"wont\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you had / you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"gonna\": \"going to\",\n    \"wanna\": \"want to\",\n    \"'t\": \"not\",\n    \"'s\": \"is\",\n    \"'ll\": \"will\",\n    \"'d\": \"would\",\n    \"'re\": \"are\",\n    \"'ve\": \"have\",\n    \"w/\": \"with\",\n    \"w/o\": \"without\",\n    \"&quot;\": \"'\",\n    \"&amp;\": \"and\",\n    \"&lt;\": \"less than\"\n    }\n    \n    # change dictionary to lower case\n    contractions = dict((k.lower(), v.lower()) for k,v in contractions.items())\n\n    # tokenize words\n    words = re.split(r\"\\s+|\\.\", text.lower())\n    \n    # expand contractions\n    new_words = []\n    for word in words:\n        if(word in contractions.keys()):\n            new_words.append(contractions[word])\n            #print(word, \"changed to\", contractions[word])\n            pass\n        else:\n            new_words.append(word)\n            pass\n        pass\n    text = ' '.join(new_words)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_train['clean_tweet'] = tweets_train.text.apply(lambda tweet: clean_tweet(tweet))\ntweets_test['clean_tweet'] = tweets_test.text.apply(lambda tweet: clean_tweet(tweet))\n\n# split data into train and validation\nX = tweets_train.drop('sentiment', axis=1).values\ny = tweets_train.sentiment.values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=4)\n\n# concatenate X and y for train and validation sets\ndef make_dataframe(X, y, X_names, y_name):\n    '''\n    Create a pandas dataframe using features (X) and target (y) variable.\n    Input:\n        X: A 2D array of features\n        y: the target variable\n        X_names: names of features in the order that they are present in X\n        y_name: name of the target variable\n    \n    Output:\n        df: pandas dataframe object\n    '''\n    X = pd.DataFrame(X, columns=X_names)\n    y = pd.Series(y, name=y_name)\n    df = pd.concat([X, y], axis=1)\n    return df\n\nX_names = ['textID', 'text', 'selected_text', 'clean_tweet']\ny_name = 'sentiment'\ntweets_train = make_dataframe(X_train, y_train, X_names=X_names, y_name=y_name)\ntweets_validation = make_dataframe(X_val, y_val, X_names=X_names, y_name=y_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nANALYZE TWEETS\n'''\n\n# distribution of sentiment\ntweets_train.sentiment.value_counts(normalize=True)\n\n# create large corpus of all tweets\ndocument = []\nfor tweet in tweets_train['clean_tweet']:\n    document.append(str(tweet))\n    pass\n\ndocument = ' '.join(document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nsns.countplot(y=tweets_train.sentiment)\nplt.ylabel('Sentiment')\nplt.xlabel('Number of tweets')\nplt.title('Distribution of Sentiment')\n\n# target class distribution\nprint(tweets_train.sentiment.value_counts()/tweets_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# plots\nplt.figure(figsize=(12, 7))\ndef plot_word_frequency(words, top_n=10):\n    word_freq = FreqDist(words)                # or word_freq = Counter(text)\n    words = [element[0] for element in word_freq.most_common(top_n)]\n    frequencies = [element[1] for element in word_freq.most_common(top_n)]\n    plot = sns.barplot(words, frequencies)\n    plt.title('Most Common Words in Tweets')\n    plt.ylabel('Frequency')\n    return plot\n\nplot_word_frequency(word_tokenize(document), top_n=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_words(sentence):\n    words = sentence.split(' ')\n    return len(words)\n\nlengths = tweets_train.text.apply(lambda tweet: num_words(str(tweet)))\n\nplt.figure(figsize=(15, 7))\nsns.boxplot(lengths, tweets_train.sentiment)\nplt.xlabel('Number of words in tweet.')\nplt.ylabel('Sentiment')\nplt.title('Tweet Lengths')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nVADER\n'''\n\n!pip install vaderSentiment\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport operator\nfrom sklearn import metrics\n\n# predict label using VADER\ndef sentiment_labeler(text):\n    analyzer = SentimentIntensityAnalyzer()\n    score = analyzer.polarity_scores(text)\n    score.pop('compound')\n    label = max(score.items(), key=operator.itemgetter(1))[0]\n    return expand_label(label)\n\ndef expand_label(label):\n    expansion = {'neu': 'neutral', 'pos': 'positive', 'neg': 'negative'}\n    return expansion[label]\n\ntweets_train['vader_sentiment'] = tweets_train.text.apply(lambda text: sentiment_labeler(str(text)))\ntweets_test['vader_sentiment'] = tweets_test.text.apply(lambda text: sentiment_labeler(str(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, normalize=False):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    classes = np.unique(y_true)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        precision=4\n        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], precision)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    pass\n\n# confusion matrix on test set\ny_true = tweets_test.sentiment.values\ny_pred = tweets_test.vader_sentiment.values\n\nplt.figure(figsize=(12, 7))\nplot_confusion_matrix(y_true, y_pred, normalize=True)\n\nprint(\"Accuracy = {:.4f}\".format(metrics.accuracy_score(y_true, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\nLSTM\n'''\n\n# segregate tweets and sentiment\n#X_train = tweets_train.text.astype(str)\nX_train = tweets_train.clean_tweet.astype(str)\ny_train = tweets_train.sentiment.astype(str)\nX_train[:5]\ny_train[:5]\n\n#X_val = tweets_validation.text.astype(str)\nX_val = tweets_validation.clean_tweet.astype(str)\ny_val = tweets_validation.sentiment.astype(str)\n\n#X_test = tweets_test.text.astype(str)\nX_test = tweets_test.clean_tweet.astype(str)\ny_test = tweets_test.sentiment.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode words in tweets to integers\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(X_train)\nX_train_encoded = word_tokenizer.texts_to_sequences(X_train)\nX_val_encoded = word_tokenizer.texts_to_sequences(X_val)\nX_test_encoded = word_tokenizer.texts_to_sequences(X_test)\nX_train_encoded[:5]\n\n# look at lengths tweets\nlengths = [len(seq) for seq in X_train_encoded]\nprint(\"Length of longest sentence: {}\".format(max(lengths)))\nplt.boxplot(lengths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment2int(array, reverse=False):\n    dictionary = {'positive': 0, 'neutral': 1, 'negative': 2}\n    if reverse:\n        reverse_dictionary = {v: k for k, v in dictionary.items()}\n        int2sentiment = [reverse_dictionary[item] for item in array]\n        return int2sentiment\n    else:\n        sentiment2int = [dictionary[item] for item in array]\n        return sentiment2int\n    pass\n\ny_train_encoded = sentiment2int(y_train)\ny_val_encoded = sentiment2int(y_val)\ny_test_encoded = sentiment2int(y_test)\ny_train_encoded[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad tweets\nMAX_SEQ_LENGTH = 30  # tweets greater than 100 in length will be truncated\nX_train_padded = pad_sequences(X_train_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\nX_val_padded = pad_sequences(X_val_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\nX_test_padded = pad_sequences(X_test_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n\nprint(X_train_padded[0], \"\\n\"*3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download word2vec embeddings\nos.makedirs('../data/')\nos.chdir('../data/')\n!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n!gunzip GoogleNews-vectors-negative300.bin\nos.chdir('../working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load word2vec embeddings\npath = '../data/GoogleNews-vectors-negative300.bin'\nword2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n\nEMBEDDING_SIZE  = 300\nVOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n\n# create an empty embedding matix\nembedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n\n# create a word to index dictionary mapping\nword2id = word_tokenizer.word_index\n\n# copy vectors from word2vec model to the words present in corpus\nfor word, index in word2id.items():\n    try:\n        embedding_weights[index, :] = word2vec[word]\n    except KeyError:\n        pass\n    pass\n\n# check embedding dimension\nprint(\"Embeddings shape: {}\".format(embedding_weights.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encode sentiments\ny_train_one_hot = to_categorical(y_train_encoded)\ny_val_one_hot = to_categorical(y_val_encoded)\ny_test_one_hot = to_categorical(y_test_encoded)\n\ny_train_one_hot[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data\nX_train = X_train_padded\nX_val = X_val_padded\nX_test = X_test_padded\n\ny_train = y_train_one_hot\ny_val = y_val_one_hot\ny_test = y_test_one_hot\n\nprint(\"-\"*50)\nprint(\"TRAINING DATA\")\nprint(\"-\"*50)\nprint('Shape of input sequences: {}'.format(X_train.shape))\nprint('Shape of output sequences: {}'.format(y_train.shape))\n\nprint('\\n')\nprint(\"-\"*50)\nprint(\"VALIDATION DATA\")\nprint(\"-\"*50)\nprint('Shape of input sequences: {}'.format(X_val.shape))\nprint('Shape of output sequences: {}'.format(y_val.shape))\n\nprint('\\n')\nprint(\"-\"*50)\nprint(\"TEST DATA\")\nprint(\"-\"*50)\nprint('Shape of input sequences: {}'.format(X_test.shape))\nprint('Shape of output sequences: {}'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import regularizers\n\n# instantiate LSTM model\nNUM_CLASSES = y_train.shape[1]\nlstm_model = Sequential()\nlstm_model.add(Embedding(input_dim     = VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n                         output_dim    = EMBEDDING_SIZE,          # length of vector with which each word is represented\n                         input_length  = MAX_SEQ_LENGTH,          # length of input sequence\n                         weights       = [embedding_weights],     # word embedding matrix\n                         trainable     = False                     # True - update embeddings_weight matrix\n))\n\nlstm_model.add(LSTM(8, return_sequences=True))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(LSTM(8, return_sequences=True))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(LSTM(8, return_sequences=False))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n# compile\nlstm_model.compile(loss      =  'categorical_crossentropy',\n                   optimizer =  'adam',\n                   metrics   =  ['acc'])\n\n# model summary\nlstm_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on train set and validate on validation set\nbatch_size= 64\nepochs = 20\nclass_weights = {0: 0.32, 1: 0.4, 2: 0.28}\nlstm_training = lstm_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), class_weight=class_weights, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy vs epochs\nplt.figure(figsize=(15, 8))\nplt.plot(lstm_training.history['acc'])\nplt.plot(lstm_training.history['val_acc'])\nplt.title('LSTM Training')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc=\"lower right\")\nplt.show()\n\n# loss vs epochs\nplt.figure(figsize=(15, 8))\nplt.plot(lstm_training.history['loss'])\nplt.plot(lstm_training.history['val_loss'])\nplt.title('LSTM Training')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model on (train + validation) set\nX = np.concatenate((X_train, X_val), axis=0)\ny = np.concatenate((y_train, y_val), axis=0)\n\n# instantiate LSTM model\nNUM_CLASSES = y_train.shape[1]\nlstm_model = Sequential()\nlstm_model.add(Embedding(input_dim     = VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n                         output_dim    = EMBEDDING_SIZE,          # length of vector with which each word is represented\n                         input_length  = MAX_SEQ_LENGTH,          # length of input sequence\n                         weights       = [embedding_weights],     # word embedding matrix\n                         trainable     = False                    # True - update embeddings_weight matrix\n))\n\nlstm_model.add(LSTM(8, return_sequences=True))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(LSTM(8, return_sequences=True))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(LSTM(8, return_sequences=False))\nlstm_model.add(BatchNormalization())\n#lstm_model.add(Dropout(0.2))\n\nlstm_model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n# compile\nlstm_model.compile(loss      =  'categorical_crossentropy',\n                   optimizer =  'adam',\n                   metrics   =  ['acc'])\n\n# model summary\nlstm_model.summary()\n\n# fit model\nbatch_size= 64\nepochs = 20\nclass_weights = {0: 0.32, 1: 0.4, 2: 0.28}\nlstm_training = lstm_model.fit(X, y, batch_size=batch_size, epochs=epochs, class_weight=class_weights, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on test set\ny_pred = lstm_model.predict_classes(X_test)\ny_pred = sentiment2int(y_pred, reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.argmax(y_test, axis = 1)\ny_test = sentiment2int(y_test, reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, normalize=False):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    classes = np.unique(y_true)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        precision=4\n        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], precision)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 9))\nplot_confusion_matrix(y_test, y_pred, normalize=True)\n\nprint(\"LSTM accuracy = {:.4f}\".format(metrics.accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}